{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import sys\n",
    "import numpy.core.numeric as _numeric\n",
    "sys.modules['numpy._core.numeric'] = _numeric  # map missing path to the real one\n",
    "\n",
    "import pickle\n",
    "with open(\"csi_data.pkl\", \"rb\") as f:\n",
    "    obj = pickle.load(f)\n",
    "#with open(\"factorvae_csi300_alpha158.pkl\", \"rb\") as f:\n",
    " #   obj = pickle.load(f)"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"WMsZDoXECCbgHYvDewIgXs",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def summarize_df(df: pd.DataFrame, name: str = \"df\", sample_rows: int = 5):\n",
    "    print(f\"=== Summary for `{name}` ===\")\n",
    "    # Shape\n",
    "    print(f\"Rows: {len(df):,} | Columns: {df.shape[1]:,}\\n\")\n",
    "    \n",
    "    # Columns\n",
    "    print(\"• Columns:\")\n",
    "    print(list(df.columns))\n",
    "    print()\n",
    "    \n",
    "    # dtypes & non-null counts\n",
    "    print(\"• dtypes & non-null counts:\")\n",
    "    buf = io.StringIO()\n",
    "    df.info(buf=buf, memory_usage=\"deep\")\n",
    "    print(buf.getvalue())\n",
    "    print()\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"• Missing values by column:\")\n",
    "    na = df.isna().sum()\n",
    "    na_pct = (na \/ len(df) * 100).round(2)\n",
    "    na_tbl = (\n",
    "        pd.DataFrame({\"n_missing\": na, \"pct_missing\": na_pct})\n",
    "        .sort_values([\"pct_missing\", \"n_missing\"], ascending=False)\n",
    "    )\n",
    "    display(na_tbl)\n",
    "    \n",
    "    # Cardinality\n",
    "    print(\"\\n• Cardinality (unique values) per column:\")\n",
    "    card = df.nunique(dropna=True).sort_values(ascending=False)\n",
    "    display(card.to_frame(\"n_unique\"))\n",
    "    \n",
    "    # Descriptive stats (numeric)\n",
    "    print(\"\\n• Descriptive stats (numeric columns):\")\n",
    "    if df.select_dtypes(include=np.number).shape[1] > 0:\n",
    "        display(df.describe().T)\n",
    "    else:\n",
    "        print(\"(no numeric columns)\")\n",
    "    \n",
    "    # Descriptive stats (non-numeric)\n",
    "    print(\"\\n• Descriptive stats (non-numeric columns):\")\n",
    "    obj_cols = df.select_dtypes(exclude=np.number).columns\n",
    "    if len(obj_cols) > 0:\n",
    "        display(df[obj_cols].describe(include=\"all\").T)\n",
    "    else:\n",
    "        print(\"(no non-numeric columns)\")\n",
    "    \n",
    "    # Top categories for object\/categorical\n",
    "    if len(obj_cols) > 0:\n",
    "        print(\"\\n• Top categories (up to 5) for object\/categorical columns:\")\n",
    "        for c in obj_cols:\n",
    "            print(f\"  - {c}\")\n",
    "            display(df[c].value_counts(dropna=False).head(5).to_frame(\"count\"))\n",
    "    \n",
    "    # Sample rows\n",
    "    print(f\"\\n• Head ({sample_rows}) and Tail ({sample_rows})\")\n",
    "    display(df.head(sample_rows))\n",
    "    display(df.tail(sample_rows))\n",
    "\n",
    "# ---- Use it on your object ----\n",
    "summarize_df(obj, name=\"obj\")"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "=== Summary for `obj` ===\n",
      "Rows: 870,621 | Columns: 159\n",
      "\n",
      "• Columns:\n",
      "['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'HIGH0', 'LOW0', 'VWAP0', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'RANK5', 'RANK10', 'RANK20', 'RANK30', 'RANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60', 'Ref($close, -2)\/Ref($close, -1) - 1']\n",
      "\n",
      "• dtypes & non-null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 870621 entries, (Timestamp('2008-01-02 00:00:00'), 'SH600000') to (Timestamp('2020-09-23 00:00:00'), 'SZ300628')\n",
      "Columns: 159 entries, KMID to Ref($close, -2)\/Ref($close, -1) - 1\n",
      "dtypes: float32(158), float64(1)\n",
      "memory usage: 534.8 MB\n",
      "\n",
      "\n",
      "• Missing values by column:\n",
      "\n",
      "• Cardinality (unique values) per column:\n",
      "\n",
      "• Descriptive stats (numeric columns):\n",
      "\n",
      "• Descriptive stats (non-numeric columns):\n",
      "(no non-numeric columns)\n",
      "\n",
      "• Head (5) and Tail (5)\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>n_missing<\/th>\n",
       "      <th>pct_missing<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KMID<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KLEN<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KMID2<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KUP<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KUP2<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>Ref($close, -2)\/Ref($close, -1) - 1<\/th>\n",
       "      <td>0<\/td>\n",
       "      <td>0.0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>159 rows × 2 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>n_unique<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CORD10<\/th>\n",
       "      <td>856472<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>CORD20<\/th>\n",
       "      <td>854065<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>CORR5<\/th>\n",
       "      <td>853774<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>CORR10<\/th>\n",
       "      <td>852549<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>CORD30<\/th>\n",
       "      <td>852339<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>IMXD5<\/th>\n",
       "      <td>9<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>CNTP5<\/th>\n",
       "      <td>9<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>IMIN5<\/th>\n",
       "      <td>5<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>IMAX5<\/th>\n",
       "      <td>5<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VWAP0<\/th>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>159 rows × 1 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>count<\/th>\n",
       "      <th>mean<\/th>\n",
       "      <th>std<\/th>\n",
       "      <th>min<\/th>\n",
       "      <th>25%<\/th>\n",
       "      <th>50%<\/th>\n",
       "      <th>75%<\/th>\n",
       "      <th>max<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KMID<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.050305<\/td>\n",
       "      <td>1.236695<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.631845<\/td>\n",
       "      <td>0.000000<\/td>\n",
       "      <td>0.703944<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KLEN<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.289531<\/td>\n",
       "      <td>1.143781<\/td>\n",
       "      <td>-1.731185<\/td>\n",
       "      <td>-0.573360<\/td>\n",
       "      <td>0.000000<\/td>\n",
       "      <td>0.878632<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KMID2<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.000716<\/td>\n",
       "      <td>0.784240<\/td>\n",
       "      <td>-2.751917<\/td>\n",
       "      <td>-0.673028<\/td>\n",
       "      <td>0.000000<\/td>\n",
       "      <td>0.664717<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KUP<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.308372<\/td>\n",
       "      <td>1.129894<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.571138<\/td>\n",
       "      <td>0.000000<\/td>\n",
       "      <td>0.897638<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>KUP2<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.141023<\/td>\n",
       "      <td>0.917857<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.604351<\/td>\n",
       "      <td>0.000000<\/td>\n",
       "      <td>0.746810<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>-0.050589<\/td>\n",
       "      <td>1.190094<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.711650<\/td>\n",
       "      <td>-0.000984<\/td>\n",
       "      <td>0.632216<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>-0.055146<\/td>\n",
       "      <td>1.212688<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.709434<\/td>\n",
       "      <td>-0.002235<\/td>\n",
       "      <td>0.632267<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>-0.054862<\/td>\n",
       "      <td>1.220845<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.708033<\/td>\n",
       "      <td>-0.002740<\/td>\n",
       "      <td>0.632148<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>-0.074790<\/td>\n",
       "      <td>1.242467<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-0.720635<\/td>\n",
       "      <td>-0.004804<\/td>\n",
       "      <td>0.617355<\/td>\n",
       "      <td>3.00<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>Ref($close, -2)\/Ref($close, -1) - 1<\/th>\n",
       "      <td>870621.0<\/td>\n",
       "      <td>0.006053<\/td>\n",
       "      <td>0.998771<\/td>\n",
       "      <td>-1.718467<\/td>\n",
       "      <td>-0.858951<\/td>\n",
       "      <td>0.006070<\/td>\n",
       "      <td>0.871049<\/td>\n",
       "      <td>1.73<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>159 rows × 8 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th>KMID<\/th>\n",
       "      <th>KLEN<\/th>\n",
       "      <th>KMID2<\/th>\n",
       "      <th>KUP<\/th>\n",
       "      <th>KUP2<\/th>\n",
       "      <th>KLOW<\/th>\n",
       "      <th>KLOW2<\/th>\n",
       "      <th>KSFT<\/th>\n",
       "      <th>KSFT2<\/th>\n",
       "      <th>OPEN0<\/th>\n",
       "      <th>...<\/th>\n",
       "      <th>VSUMN10<\/th>\n",
       "      <th>VSUMN20<\/th>\n",
       "      <th>VSUMN30<\/th>\n",
       "      <th>VSUMN60<\/th>\n",
       "      <th>VSUMD5<\/th>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <th>Ref($close, -2)\/Ref($close, -1) - 1<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>datetime<\/th>\n",
       "      <th>instrument<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2008-01-02<\/th>\n",
       "      <th>SH600000<\/th>\n",
       "      <td>0.571643<\/td>\n",
       "      <td>1.800068<\/td>\n",
       "      <td>0.253804<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.947448<\/td>\n",
       "      <td>2.712642<\/td>\n",
       "      <td>0.656584<\/td>\n",
       "      <td>0.216772<\/td>\n",
       "      <td>0.095455<\/td>\n",
       "      <td>-0.565717<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.488654<\/td>\n",
       "      <td>-0.242539<\/td>\n",
       "      <td>-0.455504<\/td>\n",
       "      <td>0.413246<\/td>\n",
       "      <td>-0.005917<\/td>\n",
       "      <td>-0.488654<\/td>\n",
       "      <td>0.242539<\/td>\n",
       "      <td>0.455504<\/td>\n",
       "      <td>-0.413246<\/td>\n",
       "      <td>1.355271<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600004<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.715257<\/td>\n",
       "      <td>1.435924<\/td>\n",
       "      <td>-0.632512<\/td>\n",
       "      <td>-0.887051<\/td>\n",
       "      <td>-1.055251<\/td>\n",
       "      <td>-1.075074<\/td>\n",
       "      <td>2.626281<\/td>\n",
       "      <td>1.184890<\/td>\n",
       "      <td>-2.985152<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-1.918234<\/td>\n",
       "      <td>-2.176609<\/td>\n",
       "      <td>-2.156992<\/td>\n",
       "      <td>0.635146<\/td>\n",
       "      <td>1.819834<\/td>\n",
       "      <td>1.918233<\/td>\n",
       "      <td>2.176609<\/td>\n",
       "      <td>2.156992<\/td>\n",
       "      <td>-0.635147<\/td>\n",
       "      <td>-0.187365<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600006<\/th>\n",
       "      <td>0.698338<\/td>\n",
       "      <td>0.598130<\/td>\n",
       "      <td>0.470045<\/td>\n",
       "      <td>0.197643<\/td>\n",
       "      <td>-0.192792<\/td>\n",
       "      <td>2.230872<\/td>\n",
       "      <td>1.214465<\/td>\n",
       "      <td>1.157374<\/td>\n",
       "      <td>0.772598<\/td>\n",
       "      <td>-0.689529<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.186897<\/td>\n",
       "      <td>-1.370251<\/td>\n",
       "      <td>-0.219659<\/td>\n",
       "      <td>1.118085<\/td>\n",
       "      <td>-0.122084<\/td>\n",
       "      <td>-0.186898<\/td>\n",
       "      <td>1.370251<\/td>\n",
       "      <td>0.219658<\/td>\n",
       "      <td>-1.118088<\/td>\n",
       "      <td>-0.843141<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600007<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.189876<\/td>\n",
       "      <td>0.174635<\/td>\n",
       "      <td>-0.652805<\/td>\n",
       "      <td>0.505547<\/td>\n",
       "      <td>-0.554218<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.045781<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-2.854785<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>2.501516<\/td>\n",
       "      <td>2.854784<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.331011<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600008<\/th>\n",
       "      <td>2.819362<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.929474<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.409912<\/td>\n",
       "      <td>-0.508966<\/td>\n",
       "      <td>-0.888653<\/td>\n",
       "      <td>1.268276<\/td>\n",
       "      <td>0.414676<\/td>\n",
       "      <td>-2.681864<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-0.867323<\/td>\n",
       "      <td>-2.383044<\/td>\n",
       "      <td>-2.802091<\/td>\n",
       "      <td>-1.540729<\/td>\n",
       "      <td>1.400767<\/td>\n",
       "      <td>0.867323<\/td>\n",
       "      <td>2.383043<\/td>\n",
       "      <td>2.802091<\/td>\n",
       "      <td>1.540731<\/td>\n",
       "      <td>-1.392744<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>5 rows × 159 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th>KMID<\/th>\n",
       "      <th>KLEN<\/th>\n",
       "      <th>KMID2<\/th>\n",
       "      <th>KUP<\/th>\n",
       "      <th>KUP2<\/th>\n",
       "      <th>KLOW<\/th>\n",
       "      <th>KLOW2<\/th>\n",
       "      <th>KSFT<\/th>\n",
       "      <th>KSFT2<\/th>\n",
       "      <th>OPEN0<\/th>\n",
       "      <th>...<\/th>\n",
       "      <th>VSUMN10<\/th>\n",
       "      <th>VSUMN20<\/th>\n",
       "      <th>VSUMN30<\/th>\n",
       "      <th>VSUMN60<\/th>\n",
       "      <th>VSUMD5<\/th>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <th>Ref($close, -2)\/Ref($close, -1) - 1<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>datetime<\/th>\n",
       "      <th>instrument<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-09-23<\/th>\n",
       "      <th>SZ300413<\/th>\n",
       "      <td>0.428904<\/td>\n",
       "      <td>0.112254<\/td>\n",
       "      <td>0.364781<\/td>\n",
       "      <td>1.316138<\/td>\n",
       "      <td>1.076447<\/td>\n",
       "      <td>0.355149<\/td>\n",
       "      <td>0.166595<\/td>\n",
       "      <td>0.022342<\/td>\n",
       "      <td>0.018848<\/td>\n",
       "      <td>-0.425547<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>2.159467<\/td>\n",
       "      <td>2.723542<\/td>\n",
       "      <td>1.241876<\/td>\n",
       "      <td>1.118376<\/td>\n",
       "      <td>-0.043427<\/td>\n",
       "      <td>-2.159466<\/td>\n",
       "      <td>-2.723541<\/td>\n",
       "      <td>-1.241878<\/td>\n",
       "      <td>-1.118378<\/td>\n",
       "      <td>1.625853<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ300433<\/th>\n",
       "      <td>0.215259<\/td>\n",
       "      <td>0.525347<\/td>\n",
       "      <td>0.149562<\/td>\n",
       "      <td>1.477475<\/td>\n",
       "      <td>0.807947<\/td>\n",
       "      <td>2.020149<\/td>\n",
       "      <td>1.136760<\/td>\n",
       "      <td>0.258749<\/td>\n",
       "      <td>0.178299<\/td>\n",
       "      <td>-0.214401<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>1.869158<\/td>\n",
       "      <td>2.232154<\/td>\n",
       "      <td>0.792592<\/td>\n",
       "      <td>0.742558<\/td>\n",
       "      <td>-1.066907<\/td>\n",
       "      <td>-1.869157<\/td>\n",
       "      <td>-2.232153<\/td>\n",
       "      <td>-0.792592<\/td>\n",
       "      <td>-0.742560<\/td>\n",
       "      <td>-0.329799<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ300498<\/th>\n",
       "      <td>0.132981<\/td>\n",
       "      <td>-1.006315<\/td>\n",
       "      <td>0.287629<\/td>\n",
       "      <td>0.359085<\/td>\n",
       "      <td>2.108934<\/td>\n",
       "      <td>-0.893320<\/td>\n",
       "      <td>-0.712529<\/td>\n",
       "      <td>-0.277052<\/td>\n",
       "      <td>-0.594305<\/td>\n",
       "      <td>-0.132646<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.054778<\/td>\n",
       "      <td>0.625424<\/td>\n",
       "      <td>1.061316<\/td>\n",
       "      <td>-0.989635<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-1.054778<\/td>\n",
       "      <td>-0.625426<\/td>\n",
       "      <td>-1.061317<\/td>\n",
       "      <td>-0.954682<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ300601<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.264779<\/td>\n",
       "      <td>0.295952<\/td>\n",
       "      <td>-0.759123<\/td>\n",
       "      <td>0.738728<\/td>\n",
       "      <td>-0.674729<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.108919<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.908998<\/td>\n",
       "      <td>-1.208914<\/td>\n",
       "      <td>0.215094<\/td>\n",
       "      <td>-1.408991<\/td>\n",
       "      <td>2.335307<\/td>\n",
       "      <td>-0.908998<\/td>\n",
       "      <td>1.208913<\/td>\n",
       "      <td>-0.215094<\/td>\n",
       "      <td>1.408992<\/td>\n",
       "      <td>-1.313411<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ300628<\/th>\n",
       "      <td>0.090940<\/td>\n",
       "      <td>-0.520556<\/td>\n",
       "      <td>0.117773<\/td>\n",
       "      <td>1.237139<\/td>\n",
       "      <td>2.081331<\/td>\n",
       "      <td>-0.363078<\/td>\n",
       "      <td>-0.147186<\/td>\n",
       "      <td>-0.449953<\/td>\n",
       "      <td>-0.577913<\/td>\n",
       "      <td>-0.090779<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.807298<\/td>\n",
       "      <td>0.263186<\/td>\n",
       "      <td>0.391178<\/td>\n",
       "      <td>0.779054<\/td>\n",
       "      <td>-0.011251<\/td>\n",
       "      <td>-0.807298<\/td>\n",
       "      <td>-0.263185<\/td>\n",
       "      <td>-0.391179<\/td>\n",
       "      <td>-0.779054<\/td>\n",
       "      <td>0.896823<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>5 rows × 159 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Hq93YMYajxTT0CSNqrZUKx",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "obj.head(-50)"
   ],
   "execution_count":10,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th>KMID<\/th>\n",
       "      <th>KLEN<\/th>\n",
       "      <th>KMID2<\/th>\n",
       "      <th>KUP<\/th>\n",
       "      <th>KUP2<\/th>\n",
       "      <th>KLOW<\/th>\n",
       "      <th>KLOW2<\/th>\n",
       "      <th>KSFT<\/th>\n",
       "      <th>KSFT2<\/th>\n",
       "      <th>OPEN0<\/th>\n",
       "      <th>...<\/th>\n",
       "      <th>VSUMN10<\/th>\n",
       "      <th>VSUMN20<\/th>\n",
       "      <th>VSUMN30<\/th>\n",
       "      <th>VSUMN60<\/th>\n",
       "      <th>VSUMD5<\/th>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <th>Ref($close, -2)\/Ref($close, -1) - 1<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>datetime<\/th>\n",
       "      <th>instrument<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2008-01-02<\/th>\n",
       "      <th>SH600000<\/th>\n",
       "      <td>0.571643<\/td>\n",
       "      <td>1.800068<\/td>\n",
       "      <td>0.253804<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.947448<\/td>\n",
       "      <td>2.712642<\/td>\n",
       "      <td>0.656584<\/td>\n",
       "      <td>0.216772<\/td>\n",
       "      <td>0.095455<\/td>\n",
       "      <td>-0.565717<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.488654<\/td>\n",
       "      <td>-0.242539<\/td>\n",
       "      <td>-0.455504<\/td>\n",
       "      <td>0.413246<\/td>\n",
       "      <td>-0.005917<\/td>\n",
       "      <td>-0.488654<\/td>\n",
       "      <td>0.242539<\/td>\n",
       "      <td>0.455504<\/td>\n",
       "      <td>-0.413246<\/td>\n",
       "      <td>1.355271<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600004<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.715257<\/td>\n",
       "      <td>1.435924<\/td>\n",
       "      <td>-0.632512<\/td>\n",
       "      <td>-0.887051<\/td>\n",
       "      <td>-1.055251<\/td>\n",
       "      <td>-1.075074<\/td>\n",
       "      <td>2.626281<\/td>\n",
       "      <td>1.184890<\/td>\n",
       "      <td>-2.985152<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-1.918234<\/td>\n",
       "      <td>-2.176609<\/td>\n",
       "      <td>-2.156992<\/td>\n",
       "      <td>0.635146<\/td>\n",
       "      <td>1.819834<\/td>\n",
       "      <td>1.918233<\/td>\n",
       "      <td>2.176609<\/td>\n",
       "      <td>2.156992<\/td>\n",
       "      <td>-0.635147<\/td>\n",
       "      <td>-0.187365<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600006<\/th>\n",
       "      <td>0.698338<\/td>\n",
       "      <td>0.598130<\/td>\n",
       "      <td>0.470045<\/td>\n",
       "      <td>0.197643<\/td>\n",
       "      <td>-0.192792<\/td>\n",
       "      <td>2.230872<\/td>\n",
       "      <td>1.214465<\/td>\n",
       "      <td>1.157374<\/td>\n",
       "      <td>0.772598<\/td>\n",
       "      <td>-0.689529<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.186897<\/td>\n",
       "      <td>-1.370251<\/td>\n",
       "      <td>-0.219659<\/td>\n",
       "      <td>1.118085<\/td>\n",
       "      <td>-0.122084<\/td>\n",
       "      <td>-0.186898<\/td>\n",
       "      <td>1.370251<\/td>\n",
       "      <td>0.219658<\/td>\n",
       "      <td>-1.118088<\/td>\n",
       "      <td>-0.843141<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600007<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.189876<\/td>\n",
       "      <td>0.174635<\/td>\n",
       "      <td>-0.652805<\/td>\n",
       "      <td>0.505547<\/td>\n",
       "      <td>-0.554218<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.045781<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-2.854785<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>-3.000000<\/td>\n",
       "      <td>2.501516<\/td>\n",
       "      <td>2.854784<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.331011<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600008<\/th>\n",
       "      <td>2.819362<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.929474<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.409912<\/td>\n",
       "      <td>-0.508966<\/td>\n",
       "      <td>-0.888653<\/td>\n",
       "      <td>1.268276<\/td>\n",
       "      <td>0.414676<\/td>\n",
       "      <td>-2.681864<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-0.867323<\/td>\n",
       "      <td>-2.383044<\/td>\n",
       "      <td>-2.802091<\/td>\n",
       "      <td>-1.540729<\/td>\n",
       "      <td>1.400767<\/td>\n",
       "      <td>0.867323<\/td>\n",
       "      <td>2.383043<\/td>\n",
       "      <td>2.802091<\/td>\n",
       "      <td>1.540731<\/td>\n",
       "      <td>-1.392744<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-09-23<\/th>\n",
       "      <th>SZ002236<\/th>\n",
       "      <td>0.725078<\/td>\n",
       "      <td>-0.689579<\/td>\n",
       "      <td>1.091397<\/td>\n",
       "      <td>-0.268482<\/td>\n",
       "      <td>0.111917<\/td>\n",
       "      <td>-1.055251<\/td>\n",
       "      <td>-1.075074<\/td>\n",
       "      <td>0.396331<\/td>\n",
       "      <td>0.591643<\/td>\n",
       "      <td>-0.715590<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.346145<\/td>\n",
       "      <td>0.575442<\/td>\n",
       "      <td>1.326658<\/td>\n",
       "      <td>1.338050<\/td>\n",
       "      <td>-2.728468<\/td>\n",
       "      <td>-0.346145<\/td>\n",
       "      <td>-0.575442<\/td>\n",
       "      <td>-1.326661<\/td>\n",
       "      <td>-1.338052<\/td>\n",
       "      <td>-0.862107<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ002241<\/th>\n",
       "      <td>-0.274159<\/td>\n",
       "      <td>0.496149<\/td>\n",
       "      <td>-0.192983<\/td>\n",
       "      <td>0.291404<\/td>\n",
       "      <td>-0.080882<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.966044<\/td>\n",
       "      <td>0.535500<\/td>\n",
       "      <td>0.373838<\/td>\n",
       "      <td>0.275500<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>1.335386<\/td>\n",
       "      <td>1.814702<\/td>\n",
       "      <td>2.288180<\/td>\n",
       "      <td>1.197364<\/td>\n",
       "      <td>0.053400<\/td>\n",
       "      <td>-1.335385<\/td>\n",
       "      <td>-1.814701<\/td>\n",
       "      <td>-2.288182<\/td>\n",
       "      <td>-1.197366<\/td>\n",
       "      <td>1.301839<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ002252<\/th>\n",
       "      <td>0.834937<\/td>\n",
       "      <td>-0.182653<\/td>\n",
       "      <td>0.845348<\/td>\n",
       "      <td>-0.299331<\/td>\n",
       "      <td>-0.305924<\/td>\n",
       "      <td>0.118062<\/td>\n",
       "      <td>0.154588<\/td>\n",
       "      <td>0.836335<\/td>\n",
       "      <td>0.839783<\/td>\n",
       "      <td>-0.822392<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.729668<\/td>\n",
       "      <td>1.414056<\/td>\n",
       "      <td>1.376508<\/td>\n",
       "      <td>0.205721<\/td>\n",
       "      <td>1.590559<\/td>\n",
       "      <td>-0.729668<\/td>\n",
       "      <td>-1.414055<\/td>\n",
       "      <td>-1.376509<\/td>\n",
       "      <td>-0.205720<\/td>\n",
       "      <td>-0.526522<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ002271<\/th>\n",
       "      <td>-0.312192<\/td>\n",
       "      <td>-0.670053<\/td>\n",
       "      <td>-0.461269<\/td>\n",
       "      <td>-0.152796<\/td>\n",
       "      <td>0.277335<\/td>\n",
       "      <td>0.125720<\/td>\n",
       "      <td>0.731110<\/td>\n",
       "      <td>-0.205317<\/td>\n",
       "      <td>-0.300858<\/td>\n",
       "      <td>0.313939<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>1.325740<\/td>\n",
       "      <td>0.708895<\/td>\n",
       "      <td>0.958861<\/td>\n",
       "      <td>1.136098<\/td>\n",
       "      <td>0.041374<\/td>\n",
       "      <td>-1.325740<\/td>\n",
       "      <td>-0.708894<\/td>\n",
       "      <td>-0.958861<\/td>\n",
       "      <td>-1.136099<\/td>\n",
       "      <td>-1.637425<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SZ002304<\/th>\n",
       "      <td>0.434145<\/td>\n",
       "      <td>0.066713<\/td>\n",
       "      <td>0.378592<\/td>\n",
       "      <td>-0.228322<\/td>\n",
       "      <td>-0.342788<\/td>\n",
       "      <td>1.981101<\/td>\n",
       "      <td>1.665740<\/td>\n",
       "      <td>0.996324<\/td>\n",
       "      <td>0.861674<\/td>\n",
       "      <td>-0.430709<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.530302<\/td>\n",
       "      <td>0.437972<\/td>\n",
       "      <td>-0.271878<\/td>\n",
       "      <td>1.743561<\/td>\n",
       "      <td>-0.070108<\/td>\n",
       "      <td>-0.530302<\/td>\n",
       "      <td>-0.437972<\/td>\n",
       "      <td>0.271878<\/td>\n",
       "      <td>-1.743564<\/td>\n",
       "      <td>0.838963<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>870571 rows × 159 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"VFROVJcESdQB79WQprzZy3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "all_stocks = obj.index.get_level_values(\"instrument\").unique().tolist()\n",
    "\n",
    "print(f\"Total unique stocks: {len(all_stocks)}\")\n",
    "print(all_stocks[:20])"
   ],
   "execution_count":9,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Total unique stocks: 682\n",
      "['SH600000', 'SH600004', 'SH600006', 'SH600007', 'SH600008', 'SH600009', 'SH600010', 'SH600011', 'SH600015', 'SH600016', 'SH600017', 'SH600018', 'SH600019', 'SH600020', 'SH600021', 'SH600022', 'SH600026', 'SH600027', 'SH600028', 'SH600029']\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"n7Gh0a8iNvKWIK6GmY3cSJ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"oINNUty6BUOzVeV6B2ozrv",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet 2"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Sheet 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# -- Sheet --\n",
    "\n",
    "# Core\n",
    "import os, math, random, warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, List\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Misc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For clean logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Python:\", os.sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 43):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(43)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "# ==== EDIT THESE IF NEEDED ====\n",
    "DATASET_PATH = Path(\"csi_data.pkl\")\n",
    "SEQ_LEN      = 1\n",
    "NUM_LATENT   = 158\n",
    "NUM_FACTORS  = 1\n",
    "HIDDEN_SIZE  = 1\n",
    "NUM_PORTFOLIO = 1\n",
    "\n",
    "# Train\/val\/test windows from your description\n",
    "TRAIN_START = \"2010-01-01\"\n",
    "TRAIN_END   = \"2017-12-31\"\n",
    "\n",
    "VAL_START   = \"2018-01-03\"\n",
    "VAL_END     = \"2018-12-29\"\n",
    "\n",
    "TEST_START  = \"2019-01-02\"\n",
    "TEST_END    = \"2020-09-20\" \n",
    "\n",
    "LR           = 1e-3\n",
    "EPOCHS       = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n",
    "\n",
    "def rankic(df: pd.DataFrame, label_col=\"LABEL0\", pred_col=\"Pred\") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-day RankIC and overall mean\/IR.\"\"\"\n",
    "    dates = df.index.get_level_values(0).unique()\n",
    "    daily = []\n",
    "    for d in dates:\n",
    "        dd = df.loc[d]\n",
    "        ric, _ = spearmanr(dd[label_col].rank(), dd[pred_col].rank())\n",
    "        daily.append({\"datetime\": d, \"RankIC\": ric})\n",
    "    daily_df = pd.DataFrame(daily).set_index(\"datetime\").sort_index()\n",
    "    mean = daily_df[\"RankIC\"].mean()\n",
    "    std  = daily_df[\"RankIC\"].std()\n",
    "    ir   = (mean \/ std) if std and std == std else np.nan\n",
    "    summary = pd.DataFrame({\"RankIC\":[mean], \"RankIC_IR\":[ir]})\n",
    "    return daily_df, summary\n",
    "import bisect\n",
    "\n",
    "def np_ffill(arr: np.ndarray) -> np.ndarray:\n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask, np.arange(mask.shape[0]), 0)\n",
    "    np.maximum.accumulate(idx, axis=0, out=idx)\n",
    "    return arr[idx]\n",
    "\n",
    "class TSDataSampler:\n",
    "    \"\"\"\n",
    "    Time-series sampler for a MultiIndex (datetime, instrument) DataFrame.\n",
    "    Returns windows of length step_len per instrument for a given date.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\", dtype=None, flt_data=None):\n",
    "        assert data.index.names == [\"datetime\", \"instrument\"]\n",
    "        self.step_len = step_len\n",
    "        self.fillna_type = fillna_type\n",
    "        self.data = data.sort_index()\n",
    "        self.data_arr = data.to_numpy(dtype=dtype)\n",
    "        self.data_arr = np.append(self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype), axis=0)\n",
    "        self.nan_idx = -1  # last row is NaNs\n",
    "        self.idx_df, self.idx_map = self.build_index(self.data)\n",
    "        self.data_index = self.data.index\n",
    "\n",
    "        if flt_data is not None:\n",
    "            flt = flt_data.reindex(self.data_index).fillna(False).astype(bool)\n",
    "            self.idx_map = self._flt_idx_map(flt, self.idx_map)\n",
    "            self.data_index = self.data_index[flt]\n",
    "\n",
    "        self.idx_map = self._idx_map2arr(self.idx_map)\n",
    "        self.start_idx, self.end_idx = self.data_index.slice_locs(start=pd.Timestamp(start), end=pd.Timestamp(end))\n",
    "        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_index(data: pd.DataFrame) -> tuple:\n",
    "        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object).unstack()\n",
    "        idx_df = idx_df.sort_index().sort_index(axis=1)\n",
    "        idx_map = {}\n",
    "        for _, row in idx_df.iterrows():\n",
    "            for j, real_idx in enumerate(row):\n",
    "                if not np.isnan(real_idx):\n",
    "                    idx_map[real_idx] = (idx_df.index.get_loc(row.name), j)\n",
    "        return idx_df, idx_map\n",
    "\n",
    "    @staticmethod\n",
    "    def _idx_map2arr(idx_map):\n",
    "        dtype = np.int32\n",
    "        NO = (np.iinfo(dtype).max, np.iinfo(dtype).max)\n",
    "        max_idx = max(idx_map.keys())\n",
    "        arr_map = [idx_map.get(i, NO) for i in range(max_idx + 1)]\n",
    "        return np.array(arr_map, dtype=dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def _flt_idx_map(flt_data, idx_map):\n",
    "        idx = 0\n",
    "        new_idx_map = {}\n",
    "        for i, exist in enumerate(flt_data):\n",
    "            if exist:\n",
    "                new_idx_map[idx] = idx_map[i]\n",
    "                idx += 1\n",
    "        return new_idx_map\n",
    "\n",
    "    def get_index(self):\n",
    "        return self.data_index[self.start_idx:self.end_idx]\n",
    "\n",
    "    def _rowcol_from_idx(self, idx) -> tuple:\n",
    "        if isinstance(idx, (int, np.integer)):\n",
    "            real_idx = self.start_idx + idx\n",
    "            if self.start_idx <= real_idx < self.end_idx:\n",
    "                i, j = self.idx_map[real_idx]\n",
    "            else:\n",
    "                raise KeyError(f\"{real_idx} out of bounds\")\n",
    "        elif isinstance(idx, tuple):\n",
    "            date, inst = idx\n",
    "            date = pd.Timestamp(date)\n",
    "            i = bisect.bisect_right(self.idx_df.index, date) - 1\n",
    "            j = bisect.bisect_left(self.idx_df.columns, inst)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return i, j\n",
    "\n",
    "    def _window_indices(self, row: int, col: int) -> np.ndarray:\n",
    "        indices = self.idx_arr[max(row - self.step_len + 1, 0): row + 1, col]\n",
    "        if len(indices) < self.step_len:\n",
    "            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n",
    "        if self.fillna_type == \"ffill\":\n",
    "            indices = np_ffill(indices)\n",
    "        elif self.fillna_type == \"ffill+bfill\":\n",
    "            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n",
    "        else:\n",
    "            assert self.fillna_type == \"none\"\n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, (list, np.ndarray)):\n",
    "            windows = [self._window_indices(*self._rowcol_from_idx(i)) for i in idx]\n",
    "            indices = np.concatenate(windows)\n",
    "        else:\n",
    "            indices = self._window_indices(*self._rowcol_from_idx(idx))\n",
    "\n",
    "        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n",
    "        data = self.data_arr[indices]\n",
    "        actual_idx = self.data_index[indices]\n",
    "        if isinstance(idx, (list, np.ndarray)):\n",
    "            data = data.reshape(-1, self.step_len, *data.shape[1:])\n",
    "        return data, actual_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx\n",
    "\n",
    "\n",
    "class TSDatasetH(Dataset):\n",
    "    def __init__(self, data, step_len=1, **kwargs):\n",
    "        self.step_len = step_len\n",
    "        self.data = data\n",
    "        self.sampler = TSDataSampler(data=data, step_len=step_len, **kwargs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sampler[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler)\n",
    "\n",
    "\n",
    "class DateGroupedBatchSampler(Sampler):\n",
    "    \"\"\"Yield one batch per date (all instruments that date).\"\"\"\n",
    "    def __init__(self, data_source: TSDatasetH, shuffle: bool = False):\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        self.grouped_indices = self._group_indices()\n",
    "\n",
    "    def _group_indices(self):\n",
    "        start_idx = self.data_source.sampler.start_idx\n",
    "        end_idx = self.data_source.sampler.end_idx\n",
    "        data_index = self.data_source.sampler.data_index[start_idx:end_idx]\n",
    "        ser = pd.Series(range(len(data_index)), index=data_index.get_level_values(\"datetime\"))\n",
    "        grouped = ser.groupby(level=\"datetime\").apply(list).values\n",
    "        return list(grouped)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.grouped_indices)\n",
    "        for group in self.grouped_indices:\n",
    "            yield group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped_indices)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    data, indices = zip(*batch)\n",
    "    data = torch.utils.data.dataloader.default_collate(data)\n",
    "    indices = [list(ix) for ix in indices]\n",
    "    return data, indices\n",
    "\n",
    "\n",
    "def init_data_loader(df, step_len, shuffle, start, end, select_feature=None):\n",
    "    if select_feature is not None:\n",
    "        df = df[select_feature]\n",
    "    ds = TSDatasetH(df, step_len=step_len, start=start, end=end, fillna_type=\"ffill+bfill\")\n",
    "    sampler = DateGroupedBatchSampler(ds, shuffle=shuffle)\n",
    "    dl = DataLoader(ds, batch_sampler=sampler, collate_fn=custom_collate_fn, pin_memory=True)\n",
    "    return dl\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_pickle(\"csi_data.pkl\").copy()\n",
    "\n",
    "#dataset = pd.read_pickle(\"\/data\/workspace_files\/sp500_data_20250106_20250126.pkl\").copy()\n",
    "\n",
    "# Keep only the 158 features + 1 label (already your format).\n",
    "# Rename last column to LABEL0 (the repo convention)\n",
    "dataset.rename(columns={dataset.columns[-1]: \"LABEL0\"}, inplace=True)\n",
    "\n",
    "# Basic checks\n",
    "print(dataset.shape, \"rows x cols\")\n",
    "print(\"Index names:\", dataset.index.names)\n",
    "print(\"Date range:\", dataset.index.get_level_values(0).min(), \"→\", dataset.index.get_level_values(0).max())\n",
    "print(\"Unique dates:\", dataset.index.get_level_values(0).nunique())\n",
    "print(\"Unique instruments:\", dataset.index.get_level_values(1).nunique())\n",
    "\n",
    "# Peek\n",
    "display(dataset.head(3))\n",
    "\n",
    "%pip install -q --upgrade \"numpy>=2.0,<3.0\" \"pandas>=2.2\"\n",
    "import numpy as np, pandas as pd\n",
    "print(\"NumPy:\", np.__version__, \"Pandas:\", pd.__version__)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_latent: int, hidden_size: int, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.normalize = nn.LayerNorm(num_latent)\n",
    "        self.linear = nn.Linear(num_latent, num_latent)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(num_latent, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, T, F)\n",
    "        x = self.normalize(x)\n",
    "        x = self.act(self.linear(x))\n",
    "        h, _ = self.gru(x)\n",
    "        return h[:, -1, :]  # (N, H)\n",
    "\n",
    "\n",
    "class FactorEncoder(nn.Module):\n",
    "    def __init__(self, num_factors: int, num_portfolio: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_portfolio)\n",
    "        self.softmax = nn.Softmax(dim=0)  # cross-sectional softmax (N dimension)\n",
    "        self.mu = nn.Linear(num_portfolio, num_factors)\n",
    "        self.sigma = nn.Linear(num_portfolio, num_factors)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent, returns):\n",
    "        # stock_latent: (N,H), returns: (N,1)\n",
    "        w = self.softmax(self.linear(stock_latent))  # (N,M)\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(1)\n",
    "        port_ret = torch.mm(w.T, returns)  # (M,1)\n",
    "        m = self.mu(port_ret.squeeze(1))\n",
    "        s = self.softplus(self.sigma(port_ret.squeeze(1)))\n",
    "        return m, s\n",
    "\n",
    "\n",
    "class AlphaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.mu = nn.Linear(hidden_size, 1)\n",
    "        self.sig = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        h = self.act(self.fc(stock_latent))\n",
    "        mu = self.mu(h)\n",
    "        sig = self.softplus(self.sig(h))\n",
    "        return mu, sig\n",
    "\n",
    "\n",
    "class BetaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_factors: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        return self.fc(stock_latent)  # (N,K)\n",
    "\n",
    "\n",
    "class FactorDecoder(nn.Module):\n",
    "    def __init__(self, alpha_layer: AlphaLayer, beta_layer: BetaLayer):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha_layer\n",
    "        self.beta = beta_layer\n",
    "\n",
    "    def forward(self, stock_latent, factor_mu, factor_sigma, return_mu_sigma=False, sample=True):\n",
    "        # alpha\/beta\n",
    "        a_mu, a_sig = self.alpha(stock_latent)     # (N,1), (N,1)\n",
    "        beta = self.beta(stock_latent)             # (N,K)\n",
    "\n",
    "        # factors\n",
    "        f_mu = factor_mu.view(-1, 1)               # (K,1)\n",
    "        f_sig = factor_sigma.view(-1, 1).clamp_min(1e-6)\n",
    "\n",
    "        # predictive mean\/var\n",
    "        mu = a_mu + torch.matmul(beta, f_mu)       # (N,1)\n",
    "        sig = torch.sqrt(a_sig**2 + torch.matmul(beta**2, f_sig**2) + 1e-6)\n",
    "\n",
    "        if return_mu_sigma:\n",
    "            return mu, sig\n",
    "        if sample:\n",
    "            eps = torch.randn_like(sig)\n",
    "            return mu + eps * sig\n",
    "        return mu  # deterministic\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.val = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        K = self.key(stock_latent)   # (N,H)\n",
    "        V = self.val(stock_latent)   # (N,H)\n",
    "        att = torch.matmul(self.query, K.T)  # (N,)\n",
    "        att = att \/ math.sqrt(K.shape[1] + 1e-6)\n",
    "        att = self.drop(att)\n",
    "        att = F.relu(att)\n",
    "        att = F.softmax(att, dim=0)\n",
    "        if torch.isnan(att).any() or torch.isinf(att).any():\n",
    "            return torch.zeros_like(V[0])\n",
    "        ctx = torch.matmul(att, V)   # (H,)\n",
    "        return ctx\n",
    "\n",
    "\n",
    "class FactorPredictor(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_factors: int):\n",
    "        super().__init__()\n",
    "        self.attn = nn.ModuleList([AttentionLayer(hidden_size) for _ in range(num_factors)])\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.mu = nn.Linear(hidden_size, 1)\n",
    "        self.sig = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        heads = [l(stock_latent) for l in self.attn]   # list of (H,)\n",
    "        h = torch.stack(heads, dim=0)                  # (K,H)\n",
    "        h = self.act(self.fc(h))\n",
    "        mu = self.mu(h).view(-1)                       # (K,)\n",
    "        sig = self.softplus(self.sig(h)).view(-1)      # (K,)\n",
    "        return mu, sig\n",
    "\n",
    "\n",
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, feature_extractor, factor_encoder, factor_decoder, factor_predictor):\n",
    "        super().__init__()\n",
    "        self.feat = feature_extractor\n",
    "        self.enc  = factor_encoder\n",
    "        self.dec  = factor_decoder\n",
    "        self.pred = factor_predictor\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_divergence(mu1, sig1, mu2, sig2):\n",
    "        # KL(q||p) for diagonal Gaussians; sum over dims\n",
    "        sig2 = torch.clamp(sig2, min=1e-6)\n",
    "        sig1 = torch.clamp(sig1, min=1e-6)\n",
    "        return (torch.log(sig2\/sig1) + (sig1**2 + (mu1 - mu2)**2)\/(2*sig2**2) - 0.5).sum()\n",
    "\n",
    "    def forward(self, x, returns):\n",
    "        # x: (N,T,F), returns: (N,1)\n",
    "        z_stock = self.feat(x)\n",
    "        post_mu, post_sig = self.enc(z_stock, returns)       # q(z|x,y)\n",
    "        recon = self.dec(z_stock, post_mu, post_sig)         # sample for training\n",
    "        prior_mu, prior_sig = self.pred(z_stock)             # p(z|x)\n",
    "\n",
    "        rec_loss = F.mse_loss(recon, returns)\n",
    "        kl = self.kl_divergence(post_mu, post_sig, prior_mu, prior_sig)\n",
    "        loss = rec_loss + kl\n",
    "        return loss, recon, post_mu, post_sig, prior_mu, prior_sig\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_mean_sigma(self, x):\n",
    "        z_stock = self.feat(x)\n",
    "        prior_mu, prior_sig = self.pred(z_stock)\n",
    "        mu, sigma = self.dec(z_stock, prior_mu, prior_sig, return_mu_sigma=True, sample=False)\n",
    "        return mu, sigma  # (N,1), (N,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = init_data_loader(\n",
    "    dataset, step_len=SEQ_LEN, shuffle=True,\n",
    "    start=TRAIN_START, end=TRAIN_END, select_feature=None\n",
    ")\n",
    "\n",
    "val_loader = init_data_loader(\n",
    "    dataset, step_len=SEQ_LEN, shuffle=False,\n",
    "    start=VAL_START, end=VAL_END, select_feature=None\n",
    ")\n",
    "\n",
    "test_loader = init_data_loader(\n",
    "    dataset, step_len=SEQ_LEN, shuffle=False,\n",
    "    start=TEST_START, end=TEST_END, select_feature=None\n",
    ")\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n",
    "\n",
    "feature_extractor = FeatureExtractor(NUM_LATENT, HIDDEN_SIZE)\n",
    "factor_encoder    = FactorEncoder(NUM_FACTORS, NUM_PORTFOLIO, HIDDEN_SIZE)\n",
    "alpha_layer       = AlphaLayer(HIDDEN_SIZE)\n",
    "beta_layer        = BetaLayer(HIDDEN_SIZE, NUM_FACTORS)\n",
    "factor_decoder    = FactorDecoder(alpha_layer, beta_layer)\n",
    "factor_predictor  = FactorPredictor(HIDDEN_SIZE, NUM_FACTORS)\n",
    "\n",
    "model = FactorVAE(feature_extractor, factor_encoder, factor_decoder, factor_predictor).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "T_max = len(train_loader) * EPOCHS if len(train_loader) > 0 else EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())  # parameter count\n",
    "\n",
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Stop training when the monitored metric stops improving.\n",
    "    - mode='min' for losses; patience = epochs to wait after last improvement\n",
    "    - min_delta = required improvement amount\n",
    "    - restore_best=True will keep best weights in memory and restore on stop\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=6, min_delta=1e-4, mode='min', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def _is_better(self, current, best):\n",
    "        if best is None:\n",
    "            return True\n",
    "        if self.mode == 'min':\n",
    "            return (best - current) > self.min_delta\n",
    "        else:\n",
    "            return (current - best) > self.min_delta\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        \"\"\"\n",
    "        Returns True if training should stop.\n",
    "        \"\"\"\n",
    "        if self.best is None or self._is_better(metric, self.best):\n",
    "            self.best = metric\n",
    "            self.num_bad = 0\n",
    "            if self.restore_best:\n",
    "                # keep best weights in memory\n",
    "                self.best_state = copy.deepcopy(model.state_dict())\n",
    "            return False\n",
    "\n",
    "        self.num_bad += 1\n",
    "        return self.num_bad > self.patience\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None, device=DEVICE, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    with tqdm(total=len(loader), desc=\"Train\") as pbar:\n",
    "        for batch, _ in loader:\n",
    "            x = batch[:, :, :-1].to(device).float()  # (N,T,158)\n",
    "            y = batch[:, :, -1].to(device).float()\n",
    "            y = y[:, -1].unsqueeze(1)                # (N,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, *_ = model(x, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping helps on tiny\/noisy sets\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                # keep per-batch cosine schedule if you’re using CosineAnnealingLR\n",
    "                scheduler.step()\n",
    "\n",
    "            total += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    return total \/ max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    with tqdm(total=len(loader), desc=\"Valid\") as pbar:\n",
    "        for batch, _ in loader:\n",
    "            x = batch[:, :, :-1].to(device).float()\n",
    "            y = batch[:, :, -1].to(device).float()\n",
    "            y = y[:, -1].unsqueeze(1)\n",
    "\n",
    "            loss, *_ = model(x, y)\n",
    "            total += loss.item()\n",
    "            pbar.update(1)\n",
    "    return total \/ max(1, len(loader))\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_path = \"factorvae_sp500_best.pt\"\n",
    "\n",
    "# patience=6 means: stop if no val improvement for 6 consecutive epochs\n",
    "early = EarlyStopping(patience=6, min_delta=1e-4, mode='min', restore_best=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, scheduler, DEVICE, grad_clip=1.0)\n",
    "    va = validate(model, val_loader, DEVICE)\n",
    "    print(f\"Epoch {epoch:03d} | train {tr:.6f} | val {va:.6f}\")\n",
    "\n",
    "    # Track best & save\n",
    "    if va < best_val - 1e-4:\n",
    "        best_val = va\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  ↳ saved new best → {best_path}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if early.step(va, model):\n",
    "        print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        if early.restore_best and early.best_state is not None:\n",
    "            model.load_state_dict(early.best_state)\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  ↳ restored best weights and re-saved to {best_path}\")\n",
    "        break\n",
    "\n",
    "# ---- Predict on test set (deterministic mean) & compute RankIC ----\n",
    "# Uses sampler's full index to avoid nested-idx headaches.\n",
    "\n",
    "# (Re)load best checkpoint if present\n",
    "if os.path.exists(\"factorvae_sp500_best.pt\"):\n",
    "    model.load_state_dict(torch.load(\"factorvae_sp500_best.pt\", map_location=DEVICE))\n",
    "\n",
    "model.eval()\n",
    "preds = []           # list of np arrays, one per date-batch\n",
    "with torch.no_grad():\n",
    "    for batch, _ in test_loader:     # ignore `_` indices\n",
    "        x = batch[:, :, :-1].to(DEVICE).float()\n",
    "        mu, sigma = model.predict_mean_sigma(x)     # (N,1)\n",
    "        preds.append(mu.squeeze(1).cpu().numpy())   # shape (N,)\n",
    "\n",
    "# 1) concatenate predictions\n",
    "pred_arr = np.concatenate(preds, axis=0)           # total length = sum over all dates\n",
    "\n",
    "# 2) get the *ordered* full MultiIndex for the test window\n",
    "mi_full = list(test_loader.dataset.sampler.get_index())  # list of (datetime, instrument) tuples\n",
    "\n",
    "# 3) align index to preds by slicing sequentially\n",
    "mi_seq = []\n",
    "cursor = 0\n",
    "for arr in preds:\n",
    "    n = int(arr.shape[0])\n",
    "    mi_seq.extend(mi_full[cursor: cursor + n])\n",
    "    cursor += n\n",
    "\n",
    "# 4) sanity check and build DataFrame\n",
    "assert len(mi_seq) == pred_arr.shape[0], f\"Index ({len(mi_seq)}) vs preds ({pred_arr.shape[0]}) mismatch\"\n",
    "mi = pd.MultiIndex.from_tuples(mi_seq, names=[\"datetime\",\"instrument\"])\n",
    "pred_df = pd.DataFrame({\"Pred\": pred_arr}, index=mi).sort_index()\n",
    "\n",
    "# 5) join with labels and compute RankIC\n",
    "labels = dataset[[\"LABEL0\"]].copy()\n",
    "eval_df = labels.join(pred_df, how=\"inner\")\n",
    "\n",
    "daily_rankic, summary = rankic(eval_df, label_col=\"LABEL0\", pred_col=\"Pred\")\n",
    "display(daily_rankic)\n",
    "display(summary)\n",
    "\n",
    "# After you have pred_df with index (datetime, instrument)\n",
    "labels_aligned = (\n",
    "    dataset[[\"LABEL0\"]]\n",
    "    .groupby(level=\"instrument\")\n",
    "    .shift(-2)                        # align label to prediction date\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "eval_df = labels_aligned.join(pred_df, how=\"inner\")\n",
    "daily_rankic, summary = rankic(eval_df, label_col=\"LABEL0\", pred_col=\"Pred\")\n",
    "#display(daily_rankic)\n",
    "display(summary)\n",
    "\n",
    "# ===== Linear dynamic baseline (fast) =====\n",
    "import math, os, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- tiny helpers ---\n",
    "def inverse_normalize_labels(y: torch.Tensor) -> torch.Tensor:\n",
    "    # y: (N,1) --- per-date batch\n",
    "    N = y.shape[0]\n",
    "    # ranks in [1..N]\n",
    "    ranks = torch.argsort(torch.argsort(y.squeeze(1))) + 1\n",
    "    u = (ranks.float() - 0.5) \/ float(N)\n",
    "    z = math.sqrt(2.0) * torch.erfinv(2.0*u - 1.0)\n",
    "    return z.view(-1,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_spearman(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    # y_true,y_pred: (N,1) for one date\n",
    "    a = y_true.squeeze(1).cpu().numpy()\n",
    "    b = y_pred.squeeze(1).cpu().numpy()\n",
    "    if np.std(b) == 0:\n",
    "        return 0.0\n",
    "    r, _ = spearmanr(a, b)\n",
    "    return 0.0 if (r != r) else float(r)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=6, mode='max', min_delta=1e-4):\n",
    "        self.patience, self.mode, self.min_delta = patience, mode, min_delta\n",
    "        self.best, self.bad = None, 0\n",
    "        self.best_state = None\n",
    "    def step(self, metric, model):\n",
    "        if self.best is None:\n",
    "            self.best = metric\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            return False\n",
    "        improve = (metric > self.best + self.min_delta) if self.mode=='max' else (metric < self.best - self.min_delta)\n",
    "        if improve:\n",
    "            self.best = metric\n",
    "            self.bad = 0\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        else:\n",
    "            self.bad += 1\n",
    "        return self.bad > self.patience\n",
    "\n",
    "# --- model: linear on last time step features ---\n",
    "class LinearDFM(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "    def forward(self, x):          # x: (N,T,F)\n",
    "        last = x[:, -1, :]         # (N,F)\n",
    "        return self.fc(last)       # (N,1)\n",
    "\n",
    "# ===== train \/ validate \/ test =====\n",
    "def train_linear(model, optimizer, loaders, device, epochs=50, weight_decay=1e-5):\n",
    "    train_loader, val_loader = loaders\n",
    "    early = EarlyStopping(patience=8, mode='max')\n",
    "    best_path = \"baseline_linear_best.pt\"\n",
    "    for ep in range(1, epochs+1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for batch, _ in tqdm(train_loader, desc=f\"Linear Train {ep}\", leave=False):\n",
    "            x = batch[:, :, :-1].to(device).float()\n",
    "            y = batch[:, :, -1].to(device).float()\n",
    "            y = inverse_normalize_labels(y[:, -1].unsqueeze(1))\n",
    "            pred = model(x)\n",
    "            loss = F.mse_loss(pred, y)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "        tr_loss \/= max(1, len(train_loader))\n",
    "\n",
    "        # ---- validate rankIC\n",
    "        model.eval()\n",
    "        ric_sum, n_batches = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch, _ in tqdm(val_loader, desc=f\"Linear Valid {ep}\", leave=False):\n",
    "                x = batch[:, :, :-1].to(device).float()\n",
    "                y = batch[:, :, -1].to(device).float()\n",
    "                y = inverse_normalize_labels(y[:, -1].unsqueeze(1))\n",
    "                pred = model(x)\n",
    "                ric_sum += batch_spearman(y, pred)\n",
    "                n_batches += 1\n",
    "        val_rankic = ric_sum \/ max(1, n_batches)\n",
    "\n",
    "        print(f\"Epoch {ep:03d} | train_loss {tr_loss:.5f} | val_RankIC {val_rankic:+.4f}\")\n",
    "\n",
    "        if early.step(val_rankic, model):\n",
    "            print(f\"Early stop at epoch {ep}. Restoring best...\")\n",
    "            model.load_state_dict(early.best_state)\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            break\n",
    "        torch.save(model.state_dict(), best_path)  # always keep last\/best\n",
    "    # restore best if not already\n",
    "    if early.best_state is not None:\n",
    "        model.load_state_dict(early.best_state)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_test_rankic(model, test_loader, device):\n",
    "    model.eval()\n",
    "    ric_list = []\n",
    "    for batch, _ in tqdm(test_loader, desc=\"Linear Test\", leave=False):\n",
    "        x = batch[:, :, :-1].to(device).float()\n",
    "        y = batch[:, :, -1].to(device).float()\n",
    "        y = inverse_normalize_labels(y[:, -1].unsqueeze(1))\n",
    "        pred = model(x)\n",
    "        ric_list.append(batch_spearman(y, pred))\n",
    "    ric = float(np.mean(ric_list))\n",
    "    ir  = float(ric \/ (np.std(ric_list)+1e-12))\n",
    "    print(f\"[Linear] Test RankIC={ric:+.4f} | IR={ir:+.4f}\")\n",
    "    return ric, ir\n",
    "\n",
    "# ===== run it (assumes train_loader\/val_loader\/test_loader exist) =====\n",
    "lin = LinearDFM(NUM_LATENT).to(DEVICE)\n",
    "opt = torch.optim.Adam(lin.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "train_linear(lin, opt, (train_loader, val_loader), DEVICE, epochs=EPOCHS)\n",
    "eval_test_rankic(lin, test_loader, DEVICE)"
   ],
   "execution_count":11,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Python: 3.11.10 (main, Aug  6 2025, 09:13:17) [GCC 11.4.0]\n",
      "Torch: 2.7.0+cu126\n",
      "CUDA available: True\n",
      "(870621, 159) rows x cols\n",
      "Index names: ['datetime', 'instrument']\n",
      "Date range: 2008-01-02 00:00:00 → 2020-09-23 00:00:00\n",
      "Unique dates: 3046\n",
      "Unique instruments: 682\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\r\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.2 which is incompatible.\r\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "NumPy: 1.26.4 Pandas: 2.2.3\n",
      "Epoch 001 | train 1.398899 | val 1.066486\n",
      "  ↳ saved new best → factorvae_sp500_best.pt\n",
      "Epoch 001 | train_loss 1.01157 | val_RankIC +0.0260\n",
      "[Linear] Test RankIC=+0.0299 | IR=+0.1928\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th>KMID<\/th>\n",
       "      <th>KLEN<\/th>\n",
       "      <th>KMID2<\/th>\n",
       "      <th>KUP<\/th>\n",
       "      <th>KUP2<\/th>\n",
       "      <th>KLOW<\/th>\n",
       "      <th>KLOW2<\/th>\n",
       "      <th>KSFT<\/th>\n",
       "      <th>KSFT2<\/th>\n",
       "      <th>OPEN0<\/th>\n",
       "      <th>...<\/th>\n",
       "      <th>VSUMN10<\/th>\n",
       "      <th>VSUMN20<\/th>\n",
       "      <th>VSUMN30<\/th>\n",
       "      <th>VSUMN60<\/th>\n",
       "      <th>VSUMD5<\/th>\n",
       "      <th>VSUMD10<\/th>\n",
       "      <th>VSUMD20<\/th>\n",
       "      <th>VSUMD30<\/th>\n",
       "      <th>VSUMD60<\/th>\n",
       "      <th>LABEL0<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>datetime<\/th>\n",
       "      <th>instrument<\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2008-01-02<\/th>\n",
       "      <th>SH600000<\/th>\n",
       "      <td>0.571643<\/td>\n",
       "      <td>1.800068<\/td>\n",
       "      <td>0.253804<\/td>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>0.947448<\/td>\n",
       "      <td>2.712642<\/td>\n",
       "      <td>0.656584<\/td>\n",
       "      <td>0.216772<\/td>\n",
       "      <td>0.095455<\/td>\n",
       "      <td>-0.565717<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.488654<\/td>\n",
       "      <td>-0.242539<\/td>\n",
       "      <td>-0.455504<\/td>\n",
       "      <td>0.413246<\/td>\n",
       "      <td>-0.005917<\/td>\n",
       "      <td>-0.488654<\/td>\n",
       "      <td>0.242539<\/td>\n",
       "      <td>0.455504<\/td>\n",
       "      <td>-0.413246<\/td>\n",
       "      <td>1.355271<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600004<\/th>\n",
       "      <td>3.000000<\/td>\n",
       "      <td>1.715257<\/td>\n",
       "      <td>1.435924<\/td>\n",
       "      <td>-0.632512<\/td>\n",
       "      <td>-0.887051<\/td>\n",
       "      <td>-1.055251<\/td>\n",
       "      <td>-1.075074<\/td>\n",
       "      <td>2.626281<\/td>\n",
       "      <td>1.184890<\/td>\n",
       "      <td>-2.985152<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>-1.918234<\/td>\n",
       "      <td>-2.176609<\/td>\n",
       "      <td>-2.156992<\/td>\n",
       "      <td>0.635146<\/td>\n",
       "      <td>1.819834<\/td>\n",
       "      <td>1.918233<\/td>\n",
       "      <td>2.176609<\/td>\n",
       "      <td>2.156992<\/td>\n",
       "      <td>-0.635147<\/td>\n",
       "      <td>-0.187365<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>SH600006<\/th>\n",
       "      <td>0.698338<\/td>\n",
       "      <td>0.598130<\/td>\n",
       "      <td>0.470045<\/td>\n",
       "      <td>0.197643<\/td>\n",
       "      <td>-0.192792<\/td>\n",
       "      <td>2.230872<\/td>\n",
       "      <td>1.214465<\/td>\n",
       "      <td>1.157374<\/td>\n",
       "      <td>0.772598<\/td>\n",
       "      <td>-0.689529<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>0.186897<\/td>\n",
       "      <td>-1.370251<\/td>\n",
       "      <td>-0.219659<\/td>\n",
       "      <td>1.118085<\/td>\n",
       "      <td>-0.122084<\/td>\n",
       "      <td>-0.186898<\/td>\n",
       "      <td>1.370251<\/td>\n",
       "      <td>0.219658<\/td>\n",
       "      <td>-1.118088<\/td>\n",
       "      <td>-0.843141<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>3 rows × 159 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"3a53995fd7f34fe98b2a3916810b73b7"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"YN8F4xCM3NKInf7nPKcVAQ"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"460177d6e75d4c7db75a9f05f69b60b2"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"O1cEUIV0hxnrYr0kjb4VpB"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>RankIC<\/th>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>datetime<\/th>\n",
       "      <th><\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02<\/th>\n",
       "      <td>-0.018183<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03<\/th>\n",
       "      <td>0.085388<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04<\/th>\n",
       "      <td>0.054325<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07<\/th>\n",
       "      <td>-0.052135<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08<\/th>\n",
       "      <td>0.123606<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14<\/th>\n",
       "      <td>-0.155044<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15<\/th>\n",
       "      <td>0.053940<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16<\/th>\n",
       "      <td>0.156411<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17<\/th>\n",
       "      <td>-0.116219<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18<\/th>\n",
       "      <td>-0.254085<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>416 rows × 1 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>RankIC<\/th>\n",
       "      <th>RankIC_IR<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>0.004136<\/td>\n",
       "      <td>0.042744<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>RankIC<\/th>\n",
       "      <th>RankIC_IR<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>-0.001964<\/td>\n",
       "      <td>-0.021021<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"9f27cd97f70140f68512d08c91f72df2"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"wLrxM3finucx8gOaNQXmwI"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"4ac1ab1ec81545a1a3cc9cb3729d1d9d"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"8vRobMkjTbd7by63eNuMa4"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"4c631e61107c46b9a1e51ebc79437e47"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"khqKPY4p96jDHXHWknCS5e"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/plain":[
       "(0.02990900206086925, 0.19282449845561112)"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"h2VGntPO1FLmkgKGTtOSb5",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def inverse_normalize_labels(y: torch.Tensor) -> torch.Tensor:\n",
    "    # y: (N,1) --- per-date batch\n",
    "    N = y.shape[0]\n",
    "    # ranks in [1..N]\n",
    "    ranks = torch.argsort(torch.argsort(y.squeeze(1))) + 1\n",
    "    u = (ranks.float() - 0.5) \/ float(N)\n",
    "    z = math.sqrt(2.0) * torch.erfinv(2.0*u - 1.0)\n",
    "    return z.view(-1,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_spearman(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    # y_true,y_pred: (N,1) for one date\n",
    "    a = y_true.squeeze(1).cpu().numpy()\n",
    "    b = y_pred.squeeze(1).cpu().numpy()\n",
    "    if np.std(b) == 0:\n",
    "        return 0.0\n",
    "    r, _ = spearmanr(a, b)\n",
    "    return 0.0 if (r != r) else float(r)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_test_rankic(model, test_loader, device):\n",
    "    model.eval()\n",
    "    ric_list = []\n",
    "    for batch, _ in tqdm(test_loader, desc=\"Linear Test\", leave=False):\n",
    "        x = batch[:, :, :-1].to(device).float()\n",
    "        y = batch[:, :, -1].to(device).float()\n",
    "        y = inverse_normalize_labels(y[:, -1].unsqueeze(1))\n",
    "        pred = model(x)\n",
    "        ric_list.append(batch_spearman(y, pred))\n",
    "    ric = float(np.mean(ric_list))\n",
    "    ir  = float(ric \/ (np.std(ric_list)+1e-12))\n",
    "    print(f\"[Linear] Test RankIC={ric:+.4f} | IR={ir:+.4f}\")\n",
    "    return ric, ir\n",
    "\n",
    "eval_test_rankic(lin, test_loader, DEVICE)"
   ],
   "execution_count":12,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "[Linear] Test RankIC=+0.0299 | IR=+0.1928\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"ef06a32dafa74a93b91e1a751fa6bbc1"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"jmXSpeejrn56amodHZyvZT"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/plain":[
       "(0.02990900206086925, 0.19282449845561112)"
      ]
     },
     "metadata":{},
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"n4BUQSxOxoSGdSWP5mjeQr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YpITO4jThxrvKJsiUbiKcc",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "widgets":{
   "application\/vnd.jupyter.widget-state+json":{
    "version_major":2,
    "version_minor":0,
    "state":{
     "71c37960f2a742acaf4a14845f515ffb":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "689ac1ef02a04c95b4e5d6d420870ea2":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "a77c78c63b0e42a6bfaa4a6fc469c94d":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":"Train: 100%",
       "description_tooltip":null,
       "layout":"IPY_MODEL_71c37960f2a742acaf4a14845f515ffb",
       "style":"IPY_MODEL_689ac1ef02a04c95b4e5d6d420870ea2"
      }
     },
     "dc34ae0c342141d7ae6d9e9b43cf278d":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "ce7d956604de4788aec504d255de1e86":{
      "model_name":"ProgressStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "faeb03000f8b4268ab1117e7bb589186":{
      "model_name":"FloatProgressModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":1897,
       "max":1897,
       "bar_style":"success",
       "style":"IPY_MODEL_ce7d956604de4788aec504d255de1e86",
       "description_tooltip":null,
       "layout":"IPY_MODEL_dc34ae0c342141d7ae6d9e9b43cf278d"
      }
     },
     "edbf2509fa0b45bbbcf50b60b57af907":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "faa1c8aa4666439785e661cdbcb983bd":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "5454fcd9788243388d53c051df537421":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":" 1897\/1897 [27:55&lt;00:00,  1.08it\/s, loss=1.11]",
       "description_tooltip":null,
       "layout":"IPY_MODEL_edbf2509fa0b45bbbcf50b60b57af907",
       "style":"IPY_MODEL_faa1c8aa4666439785e661cdbcb983bd"
      }
     },
     "623c35f4149d4bcea93daef39330bf40":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "3a53995fd7f34fe98b2a3916810b73b7":{
      "model_name":"HBoxModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "children":[
        "IPY_MODEL_a77c78c63b0e42a6bfaa4a6fc469c94d",
        "IPY_MODEL_faeb03000f8b4268ab1117e7bb589186",
        "IPY_MODEL_5454fcd9788243388d53c051df537421"
       ],
       "layout":"IPY_MODEL_623c35f4149d4bcea93daef39330bf40"
      }
     },
     "b1de867e66684f97bb7bfcb56104b408":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "15bbf38c5eb44b16a01667f78d44fd49":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "9b96c35d125c49c2984a57a226115eae":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":"Linear Test: 100%",
       "description_tooltip":null,
       "layout":"IPY_MODEL_b1de867e66684f97bb7bfcb56104b408",
       "style":"IPY_MODEL_15bbf38c5eb44b16a01667f78d44fd49"
      }
     },
     "f7d441a305f343bda62eec1c57546b83":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "0745226036cf4ce0a8a229de29075986":{
      "model_name":"ProgressStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "14dcf8cb231f469590d7bdcab8443f12":{
      "model_name":"FloatProgressModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":416,
       "max":416,
       "style":"IPY_MODEL_0745226036cf4ce0a8a229de29075986",
       "description_tooltip":null,
       "layout":"IPY_MODEL_f7d441a305f343bda62eec1c57546b83"
      }
     },
     "ba237c3ffe08491eab34eb5434805876":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{}
     },
     "8b44f74c028d4b5b8e06daba8bc966b9":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.2.0",
       "description_width":""
      }
     },
     "bf2915fbba4e43b880a682844bd1c2c6":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "value":" 416\/416 [06:11&lt;00:00,  1.14it\/s]",
       "description_tooltip":null,
       "layout":"IPY_MODEL_ba237c3ffe08491eab34eb5434805876",
       "style":"IPY_MODEL_8b44f74c028d4b5b8e06daba8bc966b9"
      }
     },
     "9f2b035b05c74b20a9851ae8a77e4b35":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{
       "visibility":"hidden"
      }
     },
     "ef06a32dafa74a93b91e1a751fa6bbc1":{
      "model_name":"HBoxModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module_version":"1.5.0",
       "_view_module_version":"1.5.0",
       "children":[
        "IPY_MODEL_9b96c35d125c49c2984a57a226115eae",
        "IPY_MODEL_14dcf8cb231f469590d7bdcab8443f12",
        "IPY_MODEL_bf2915fbba4e43b880a682844bd1c2c6"
       ],
       "layout":"IPY_MODEL_9f2b035b05c74b20a9851ae8a77e4b35"
      }
     }
    }
   }
  },
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default_3_11",
   "packages":[],
   "report_row_ids":[],
   "report_tabs":[],
   "version":4
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}